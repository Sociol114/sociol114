{
  "hash": "1b955c1b6d8346fb66eb3824b76899c3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Problem Set 4: Statistical Learning\"\n---\n\n\n\n**Due: 5pm on Friday, Feb 28.**\n\nIdea:\n- Income prediction challenge. OLS with two predictor sets. Sample split. Which one is better?\n- OLS with all variables. G-formula for causal effect of college on earnings, in subgroups of parent college\n\n\n\nThis problem set has not yet been posted.\n\nclass website page is a possibility for data here. Effect of college on subjective social class.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknitr::opts_chunk$set(eval = FALSE)\n```\n:::\n\n\n\n**NOTE:** This problem set will be updated to explicitly involve statistical learning with more guidance, as well as some causal inference.\n\n::: {.callout-note}\nWant to see how you'll be evaluated? Check out the [rubric](https://docs.google.com/forms/d/e/1FAIpQLSfE9cKW1TdZvmNH7AUTdzL2Fd3vnQicKVECc7qW2_S4civJpg/viewform?usp=sharing)\n:::\n\nStudent identifer: [type your anonymous identifier here]\n\n- Use this [.qmd template](../assets/pset3/pset3.qmd) to complete the problem set\n- In Canvas, you will upload the PDF produced by your .qmd file\n- Put your identifier above, not your name! We want anonymous grading to be possible\n\nThis problem set is connected to the [PSID Income Prediction Challenge](../topics/prediction.qmd) from discussion.\n\n## NOTES\n\nGoals are\n\n* Sample split\n* Estimate MSE\n\n* Parametric g-formula outcome\n\n* there is no ML algorithm on this problem set nor any IPTW\n\nWould be nice to have a nonlinear confounder and a binary treatment.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n# taken from principal stratification \ndata <- readRDS(\"../data_raw/motherhood.RDS\") |>\n  select(\n    treated, employed, age = age_2, sex, race, employed_baseline, \n    educ, marital, fulltime, tenure, experience,\n    weight = w\n  ) |>\n  na.omit()\n```\n:::\n\n\n\n# Part 1: Model employment among new mothers by age\n\n(note: linear and logistic are almost identical)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# new parents vs never parents, by sex?\n\ndata |>\n  ggplot(aes(x = age, y = as.numeric(employed), color = treated)) +\n  geom_smooth(method = \"lm\") +\n  facet_wrap(~sex)\n\n# Do just among women.\n# What is MSE of model that interacts age * treated? What is MSE of model that does not?\n```\n:::\n\n\n\n# Part 2: Model selection by sample splitting\n\n# Part 3: Causal effect of motherhood\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_mothers <- data |>\n  filter(treated)\n\nsummary(lm(employed ~ age * sex, data = new_mothers))\n\nnew_mothers |>\n  ggplot(aes(x = age, y = as.numeric(employed), color = sex)) +\n  geom_smooth()\n\nfit_linear <- lm(employed ~ age, data = new_mothers)\nfit_logistic <- glm(employed ~ age, data = new_mothers, family = binomial)\nfit_loess <- loess(employed ~ age, data = new_mothers)\n\nnew_mothers |>\n  mutate(\n    linear = predict(fit_linear),\n    logistic = predict(fit_logistic, type = \"response\"),\n    loess = predict(fit_loess, type = \"response\")\n  ) |>\n  select(age, linear, logistic, loess) |>\n  pivot_longer(cols = -age) |>\n  ggplot(aes(x = age, y = value, color = name)) +\n  geom_line()\n\nnew_mothers |>\n  group_by(age) |>\n  summarize(employed = weighted.mean(employed, w = weight)) |>\n  ggplot(aes(x = age, y = employed)) +\n  geom_point()\n```\n:::\n\n\n\nPart 1) Parametric g-formula\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- lm(employed ~ treated * sex * (age + race + employed_baseline + educ + marital + fulltime + tenure + experience),\n          data = data)\n\ncoef(fit)\ncoef(fit_ridge, s = \"lambda.1se\")\n\n\n\n\nfit_ridge <- cv.glmnet(\n  x = model.matrix(\n    ~ treated * sex * (age + race + employed_baseline + educ + marital + fulltime + tenure + experience),\n    data = data\n  ),\n  y = data$employed,\n  alpha = 0\n)\n\n\n\n# Parametric g-formula\ndata |>\n  mutate(yhat1 = predict(fit, newdata = data |> mutate(treated = TRUE)),\n         yhat0 = predict(fit, newdata = data |> mutate(treated = FALSE))) |>\n  group_by(sex) |>\n  summarize(average_effect = mean(yhat1 - yhat0))\n```\n:::\n\n\n\nPart 2) Parametric g-formula with machine learning\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwomen <- data |>\n  filter(sex == \"Women\")\nfit <- causal_forest(\n  X = model.matrix(\n    ~ age + race + employed_baseline + educ + marital + fulltime + tenure + experience,\n    data = women\n  ),\n  W = women$treated,\n  Y = women$employed\n)\n\naverage_treatment_effect(fit)\n```\n:::\n\n\n\nOuttakes: Too hard\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Inverse probability weighting\nfit_a <- glm(\n  treated ~ sex * (age + race + employed_baseline + educ + marital + fulltime + tenure + experience),\n  data = data\n)\n\ndata |>\n  mutate(\n    pscore = predict(fit_a, type = \"response\"),\n    pscore = case_when(treated ~ pscore,\n                       !treated ~ 1 - pscore)\n  ) |>\n  group_by(sex, treated) |>\n  summarize(estimate = weighted.mean(employed, w = 1 / pscore)) |>\n  pivot_wider(names_from = \"treated\", values_from = \"estimate\") |>\n  mutate(average_effect = `TRUE` - `FALSE`)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata |>\n  ggplot(aes(x = g2_log_income, y = as.numeric(g3_educ == \"College\"), color = g2_educ == \"College\")) +\n  geom_smooth()\n```\n:::\n\n\n\n## Prediction challenge\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- read_csv(\"data_raw/income_challenge/for_students/learning.csv\") |>\n  mutate(across(contains(\"educ\"), \\(x) factor(x,levels = c(\"Less than high school\",\"High school\",\"Some college\",\"College\"))))\n```\n:::\n\n\n\n## 1. OLS prediction\n\nPredict `g3_log_income` given all other variables by OLS.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- lm(\n  g3_log_income ~ race + sex + \n    g1_log_income + g2_log_income + \n    g1_educ + g2_educ*g3_educ,\n  data = data\n)\n```\n:::\n\n\n\nEstimate the average causal effect of college vs high school.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata |>\n  mutate(yhat1 = predict(fit, newdata = data |> mutate(g3_educ = \"College\")),\n         yhat0 = predict(fit, newdata = data |> mutate(g3_educ = \"High school\"))) |>\n  summarize(ate = mean(yhat1 - yhat0))\n```\n:::\n\n\n\nEstimate within subgroups of parents' education\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata |>\n  mutate(yhat1 = predict(fit, newdata = data |> mutate(g3_educ = \"College\")),\n         yhat0 = predict(fit, newdata = data |> mutate(g3_educ = \"High school\"))) |>\n  group_by(g2_educ) |>\n  summarize(ate = mean(yhat1 - yhat0))\n```\n:::\n\n\n\n## 2. Causal forest prediction\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(grf)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfor_forest <- data |>\n      filter(g3_educ %in% c(\"High school\",\"College\"))\n\nfit <- causal_forest(\n  X = model.matrix(\n    ~ race + sex + g2_log_income + g2_educ,\n    data = for_forest\n  ),\n  Y = for_forest$g3_log_income,\n  W = for_forest$g3_educ == \"College\"\n)\n\nsummary(fit)\naverage_treatment_effect(fit, target.sample = \"control\")\n\n\n\nconditi\n\n# Prepare the predictor matrix\nmodel.matrix(fit)\n\nX <- X0 <- X1 <- model.matrix(\n  ~ race + sex + \n    g1_log_income + g2_log_income + \n    g1_educ + g2_educ*g3_educ,\n  data = data\n)\nX1[\"\"]\n\nX_factual <- model.matrix(\n  object = ~g3_log_income ~ race + sex,\n  data = data.frame(data)\n)\nX_treated <- model.matrix(\n  ~g3_log_income ~ race + sex + \n    g1_log_income + g2_log_income + \n    g1_educ + g2_educ*g3_educ,\n  data = data |> mutate(g3_educ = \"College\")\n)\nX_untreated <- model.matrix(\n  ~g3_log_income ~ race + sex + \n    g1_log_income + g2_log_income + \n    g1_educ + g2_educ*g3_educ,\n  data = data |> mutate(g3_educ = \"High school\")\n)\n\nfit <- regression_forest(\n  X = \n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata |>\n  group_by(g3_educ) |>\n  summarize(y = mean(g3_log_income)) |>\n  mutate(\n    g3_educ = factor(g3_educ),\n    g3_educ = fct_relevel(g3_educ, \"Less than high school\",\"High school\",\"Some college\",\"College\")\n  ) |>\n  ggplot(aes(x = g3_educ, y = y)) +\n  geom_point() +\n  scale_y_continuous(labels = function(x) scales::label_dollar()(exp(x)))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata |>\n  mutate(\n    yhat_factual = predict(fit_ols),\n    yhat_counterfactual = predict(fit_ols, newdata = data |> mutate(g3_educ = \"College\"))\n  ) |>\n  select(g2_log_income, starts_with(\"yhat\")) |>\n  pivot_longer(cols = -g2_log_income) |>\n  ggplot(aes(x = g2_log_income, y = value, color = name)) +\n  geom_line()\n```\n:::\n\n\n\n\n\n# Model g3_log_income given g2_log_income\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata |>\n  ggplot(aes(x = g2_log_income, y = g3_log_income)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F) +\n  facet_wrap(~ g3_educ)\n\nfit <- lm(g3_log_income ~ g3_educ + g2_log_income + race + sex, data = data)\n\n\n\n\n\nfit <- lm(g3_log_income ~ g2_log_income + g1_log_income, data = data)\n```\n:::\n\n\n\n## Income Prediction Challenge\n\n**Collaboration note.** This question is an individual write-up connected to your group work from discussion. We expect that the approach you tell us might be the same as that of your other group members, but your answers to these questions should be in your own words.\n\n**1.1 (5 points)** How did you choose the predictor variables you used? Correct answers might be entirely conceptual, entirely data-driven, or a mixture of both.\n\n**1.2 (5 points)** What learning algorithms or models did you consider, and how did you choose one? Correct answers might be entirely conceptual, entirely data-driven, or a mixture of both.\n\n**1.3 (20 points)** Split the `learning` data randomly into `train` and `test`. Your split can be 50-50 or another ratio. Learn in the `train` set and make predictions in the `test` set. What do you estimate for your out-of-sample mean squared error? There is no written answer here; the answer is the code and result.\n\n## Grad. Machine learning versus statistics\n\n> This question is required for grad students. It is optional for undergrads, and worth no extra credit.\n\n**20 points.** This question is about the relative gain in this problem as we move from no model to a statistical model to a machine learning model.\n\nFirst, use your `train` set to estimate 3 learners and predict in your `test` set.\n\na) No model. For every `test` observation, predict the mean of the `train` outcomes\nb) Ordinary Least Squares. Choose a set of predictors $\\vec{X}$. For every `test` observation, predict using a linear model `lm()` fit to the `train` set with the predictors $\\vec{X}$.\nc) Machine learning. Use the same set of predictors $\\vec{X}$. For every `test` observation, predict using a machine learning model fit to the `train` set with the predictors $\\vec{X}$. Your machine learning model could be a Generalized Additive Model (`gam()`), a decision tree (`rpart()`), or some other machine learning approach.\n\nReport your out-of-sample mean squared error estimates for each approach. How did mean squared error change from (a) to (b)? From (b) to (c)?\n\nInterpret what you found. To what degree does machine learning improve predictability, beyond what can be achieved by Ordinary Least Squares?\n",
    "supporting": [
      "pset4_in_progress_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}